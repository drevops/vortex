---
sidebar_label: Overview
sidebar_position: 1
---

# Continuous Integration

**Vortex** offers continuous integration configurations for GitHub Actions and
CircleCI providers that allows to automate the process of building, testing, and
deploying your site.

The workflow configuration is identical for both continuous integration
providers. You would need to choose one of them and follow the setup
instructions.

The continuous integration pipeline consists of multiple jobs executed in a
containerized environment to ensure consistency across runs.

## Trigger Conditions

The continuous integration pipeline is triggered by:

- **Push events** to the following branches:
  - `production`,
  - `main`,
  - `master`,
  - `develop`
  - `release/**`,
  - `hotfix/**`,
  - `project/**`
- **Pull requests** to these branches
- **Scheduled runs** for automatic database caching

## Workflow Structure

### 1. Database

- Runs in a [`drevops/ci-runner`](https://github.com/drevops/ci-runner) _runner_ container
- Downloads the latest DB version based on a caching strategy
- Caches database dumps to speed up the follow-up runs

### 2. Build

- Runs after the `database` job
- Uses Docker Compose to set up the environment
- Provisions a website
- Lints code (only on the first instance)
- Runs unit tests
- Runs BDD tests
- Generates code coverage reports
- Collects and stores test results and artifacts

### 3. Deployment

- Runs after successful completion of a previous `build` job
- Uses the built codebase without development dependencies from the `build` step
- Adds required secrets and environment variables
- Triggers a deployment using a router script

## Caching Strategy

Database is downloaded overnight and cached so that the next continuous integration run on the same
day uses the cached database dump.

By default, the database is cached per-branch for 24 hours. If cache is not
available, the fallback default branch is used.

:::note

Database caching is a very powerful feature that allows to speed up the
continuous integration runs on large projects with a lot of data.

In case of a project with a large database >1GB, the database import itself may
take a long time, so it may be worth looking into either packaging the database
dump into a container image (overnight) or using a sanitized database dump with
only the required data for the tests.

**Vortex** supports both creating and using a database container image with embedded
data. You may use [MariaDB data container for Drupal with database captured as Docker layers](https://github.com/drevops/mariadb-drupal-data) to create
an initial database image.

There are other tools also available for this purpose, such as [Drush GDPR Dumper](https://github.com/robiningelbrecht/drush-gdpr-dumper)
that allows to remove data from the database dump during Drush database
export command without the need for an intermediate database import step.

:::
